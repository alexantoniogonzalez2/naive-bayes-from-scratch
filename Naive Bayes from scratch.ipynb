{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes from scrath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens a data file in csv, and transform it into a usable format\n",
    "def load_data():\n",
    "    \n",
    "    # Reading file.\n",
    "    data = pd.read_csv('data/student.csv', sep=',')\n",
    "    \n",
    "    # 'Grade' is the label, the rest are the features\n",
    "    Y_data = data['Grade']\n",
    "    X_data = data.drop(columns=['Grade'])\n",
    "    \n",
    "    # It is applied factorize function to the features.\n",
    "    X_data = X_data.apply(lambda feature: pd.factorize(feature)[0])\n",
    "    \n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits a data set into a training set and hold-out test set\n",
    "def split_data(X_data,Y_data,holdout):\n",
    "    \n",
    "    # Mask is just an array with \"TRUE\" of \"FALSE\" components used to split the data set in train and test.\n",
    "    mask = np.random.rand(len(X_data)) < holdout\n",
    "    \n",
    "    X_train = X_data[mask]\n",
    "    X_test = X_data[~mask]\n",
    "    Y_train = Y_data[mask]\n",
    "    Y_test = Y_data[~mask]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function builds a supervised NB model\n",
    "def train(X_train,Y_train,alpha):\n",
    "   \n",
    "    # Number. Lengh of the train dataset.\n",
    "    train_rows = len(X_train)\n",
    "    \n",
    "    # Count of labels, stored in a dictionary.\n",
    "    count_labels = Y_train.value_counts().to_dict()\n",
    "    \n",
    "    # Priors of labels, stored in a dictionary.\n",
    "    priors = dict(map(lambda count_label: \n",
    "                           (count_label[0], count_label[1]/train_rows ), count_labels.items()))\n",
    "    \n",
    "    # Dictionary to count the values of the instances, stored as dictionary: {feature: {label: {atributte: }}}   \n",
    "    count = {feature_name: {label: {attr_value: 0 \n",
    "                                    for attr_value in X_data[feature_name].unique()} \n",
    "                            for label in labels} \n",
    "             for feature_name in feature_names}\n",
    "    \n",
    "    # New dictionary to store the likelihoods (the count will be used for One-R method).\n",
    "    likelihoods = copy.deepcopy(count)\n",
    "    \n",
    "    # Loop for counting.\n",
    "    for index, row in X_train.iterrows():\n",
    "        for feature_name in feature_names:\n",
    "            count[feature_name][Y_train[index]][row[feature_name]] += 1\n",
    "    \n",
    "    # Loop for calculate the likelihoods.\n",
    "    for feature_name in feature_names:\n",
    "        for label in labels:\n",
    "            for key in count[feature_name][label]:\n",
    "                likelihoods[feature_name][label][key] = ( (alpha + count[feature_name][label][key])/\n",
    "                                                         (alpha*len(count[feature_name][label]) + count_labels[label])\n",
    "                                                        )\n",
    "    \n",
    "    return likelihoods, priors, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(likelihoods,priors,X_test):\n",
    "    \n",
    "    # It is created a new dataframe with the same structure (we are interested in the index) and one column.\n",
    "    prediction = pd.DataFrame(index=X_test.index,columns=['Prediction'])\n",
    "    \n",
    "    # We loop for all the instances.\n",
    "    for index, row in X_test.iterrows():\n",
    "        maximum = -100\n",
    "        argmax_y = \"\"\n",
    "        \n",
    "        # For each instance is selected the best prediction.\n",
    "        for prior_val,prior_prob in priors.items():\n",
    "            y = math.log(prior_prob)\n",
    "            for feature_name in feature_names:\n",
    "                y += math.log(likelihoods[feature_name][prior_val][row[feature_name]])\n",
    "            \n",
    "            if y > maximum :\n",
    "                maximum = y\n",
    "                argmax_y = prior_val\n",
    "            \n",
    "        prediction.at[index,'Prediction'] = argmax_y\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluate a set of predictions in terms of accuracy\n",
    "def evaluate(Y_test,prediction):\n",
    "    \n",
    "    # It is received prediction dataframes with diferent names columns (used for One-R method). \n",
    "    column_name = prediction.columns[0]\n",
    "\n",
    "    test_rows = len(Y_test)\n",
    "    count = 0\n",
    "    for index in Y_test.index:\n",
    "        if Y_test[index] == prediction.at[index, column_name]:\n",
    "            count += 1\n",
    "        \n",
    "    return round(count/test_rows, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predict the class based in One-R method\n",
    "def one_r_prediction(count,X_test,Y_test):\n",
    "    \n",
    "    # It is defined a dictionary for the rules with the structure: {features: {attributes: } }\n",
    "    rules = {feature_name: {attr_value: \"\" for attr_value in X_data[feature_name].unique()} \n",
    "                            for feature_name in feature_names}\n",
    "    \n",
    "    # A dataframe for saving the predictions of each feature.\n",
    "    one_r_prediction = pd.DataFrame(index=X_test.index,columns=X_test.columns)\n",
    "    \n",
    "    # It is setted the rules. For each feature, for each attribute, is selected the attribute value with\n",
    "    # higher recurrence.\n",
    "    for feature_name in feature_names:\n",
    "        for attr_value in X_data[feature_name].unique():\n",
    "            max_value = 0\n",
    "            max_label = \"\"\n",
    "            for label in labels:\n",
    "                if count[feature_name][label][attr_value] > max_value:\n",
    "                    max_value = count[feature_name][label][attr_value]\n",
    "                    max_label = label\n",
    "            \n",
    "            rules[feature_name][attr_value] = max_label\n",
    "    \n",
    "    # According to the rules, it is predicted the label for each instance. It is estimated a prediction for\n",
    "    # each attribute.\n",
    "    for feature_name in feature_names:\n",
    "        for index, row in X_test.iterrows():\n",
    "            one_r_prediction.at[index,feature_name] = rules[feature_name][row[feature_name]]\n",
    "    \n",
    "    # It is selected the attribute that makes the best prediction, the minimum error rate.\n",
    "    min_error_rate = 1\n",
    "    selected_feature = \"\"\n",
    "    for feature_name in feature_names:\n",
    "        # It is reused the evaluation function for evaluating each feature prediction.\n",
    "        error_rate = 1 - evaluate(Y_test,one_r_prediction[feature_name].to_frame() )\n",
    "        if error_rate < min_error_rate :\n",
    "            min_error_rate = error_rate\n",
    "            selected_feature = feature_name\n",
    "        \n",
    "    return selected_feature, min_error_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for Repeated Random Subsampling reusing the functions defined above.\n",
    "def repeated_random_subsampling(samples):\n",
    "    \n",
    "    accuracy_samples = [None]*samples\n",
    "    \n",
    "    for x in range(0,samples):\n",
    "        X_train, Y_train, X_test, Y_test = split_data(X_data,Y_data,0.8)\n",
    "        rows = len(X_data)\n",
    "        labels = Y_data.unique()\n",
    "        feature_names = list(X_data.columns)\n",
    "        likelihoods,priors,count = train(X_train,Y_train,1)\n",
    "        prediction = predict(likelihoods,priors,X_test)\n",
    "        accuracy_samples[x] = evaluate(Y_test,prediction)\n",
    "        \n",
    "    return accuracy_samples, sum(accuracy_samples)/len(accuracy_samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model\n",
      "Accuracy:0.39\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_data, Y_data = load_data()\n",
    "# Split dataset with a 80-20 hold-out strategy\n",
    "X_train, Y_train, X_test, Y_test = split_data(X_data,Y_data,0.8)\n",
    "\n",
    "# General variables\n",
    "rows = len(X_data)\n",
    "labels = Y_data.unique()\n",
    "feature_names = list(X_data.columns)\n",
    "\n",
    "# Training, prediction and evaluation\n",
    "likelihoods,priors,count = train(X_train,Y_train,1)\n",
    "prediction = predict(likelihoods,priors,X_test)\n",
    "accuracy = evaluate(Y_test,prediction)\n",
    "\n",
    "print(\"Naive Bayes Model\")\n",
    "print(\"Accuracy:\"+ \"{:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "- The accuracy achieved has a value that goes between 0.3 and 0.4, varying in each execution (probably due to the hold-out strategy). Other training strategies, as Cross Validation, should avoid this issue.\n",
    "- The manual inspection is executed below. To be honest, it is very hard to find a consistent pattern. The only interesting thing is that the labels \"A+\" and \"A\" are not properly predict, probably due to there is little data from those classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions\n",
      "     school  sex  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  reason  \\\n",
      "55        1    1        0        0        0     2     2     1     1       1   \n",
      "81        0    0        0        0        0     0     0     0     1       1   \n",
      "60        0    0        0        0        0     1     2     0     2       3   \n",
      "6         0    1        1        0        0     2     2     3     1       0   \n",
      "546       0    0        0        0        0     1     2     1     1       3   \n",
      "482       0    1        0        0        0     1     1     3     2       0   \n",
      "229       1    1        0        1        0     0     0     3     4       1   \n",
      "84        1    1        1        0        0     1     1     1     1       1   \n",
      "542       0    0        1        0        0     0     1     2     2       2   \n",
      "220       0    0        1        1        0     1     1     0     1       3   \n",
      "\n",
      "     ...  romantic  famrel  freetime  goout  Dalc  Walc  health  absences  \\\n",
      "55   ...         0       4         3      4     0     3       0         0   \n",
      "81   ...         0       0         2      0     0     3       0         3   \n",
      "60   ...         0       3         0      3     0     2       4         2   \n",
      "6    ...         0       2         3      2     0     3       2         0   \n",
      "546  ...         0       0         2      3     0     4       3         1   \n",
      "482  ...         1       3         2      0     0     3       1         2   \n",
      "229  ...         1       1         2      3     0     3       2         0   \n",
      "84   ...         0       3         4      1     0     3       3         0   \n",
      "542  ...         1       3         2      0     3     4       1         1   \n",
      "220  ...         0       0         4      2     0     3       3         2   \n",
      "\n",
      "     Grade  Prediction  \n",
      "55       F           F  \n",
      "81       C           C  \n",
      "60       B           B  \n",
      "6        C           C  \n",
      "546      D           D  \n",
      "482      C           C  \n",
      "229      B           B  \n",
      "84       D           D  \n",
      "542      D           D  \n",
      "220      B           B  \n",
      "\n",
      "[10 rows x 31 columns]\n",
      "Wrong predictions\n",
      "     school  sex  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  reason  \\\n",
      "209       0    1        1        0        0     1     2     1     1       3   \n",
      "514       1    1        0        0        0     2     2     1     1       1   \n",
      "53        0    1        0        1        0     0     1     2     2       1   \n",
      "236       0    1        0        0        1     0     1     0     2       3   \n",
      "407       0    0        0        0        1     1     1     1     1       0   \n",
      "187       1    0        1        0        0     1     0     1     3       2   \n",
      "247       1    0        0        0        1     2     1     1     1       2   \n",
      "204       0    0        0        1        0     1     0     0     1       0   \n",
      "37        0    1        0        1        0     1     1     1     1       3   \n",
      "516       1    1        1        1        1     2     1     3     1       1   \n",
      "\n",
      "     ...  romantic  famrel  freetime  goout  Dalc  Walc  health  absences  \\\n",
      "209  ...         1       0         2      0     0     3       3         0   \n",
      "514  ...         0       1         0      4     0     3       0         2   \n",
      "53   ...         0       0         4      0     0     0       2         3   \n",
      "236  ...         0       3         2      2     0     3       2         2   \n",
      "407  ...         1       3         1      2     0     3       1         2   \n",
      "187  ...         0       2         4      1     0     0       0         0   \n",
      "247  ...         0       3         4      0     0     3       0         2   \n",
      "204  ...         1       3         2      0     0     2       0         0   \n",
      "37   ...         0       3         4      3     0     2       2         3   \n",
      "516  ...         1       3         2      3     0     4       0         2   \n",
      "\n",
      "     Grade  Prediction  \n",
      "209      F           B  \n",
      "514      B           D  \n",
      "53       A           B  \n",
      "236      B           A  \n",
      "407      B           C  \n",
      "187      D           F  \n",
      "247      D           C  \n",
      "204      A           C  \n",
      "37       B           C  \n",
      "516      B           D  \n",
      "\n",
      "[10 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "df = X_test.join(Y_test).join(prediction)\n",
    "df_correct = df[df.Grade==df.Prediction]\n",
    "df_wrong = df[df.Grade!=df.Prediction]\n",
    "print(\"Correct predictions\")\n",
    "print(df_correct.sample(10))\n",
    "print(\"Wrong predictions\")\n",
    "print(df_wrong.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated Random Subsampling:\n",
    "\n",
    "- This method works as the hold-out strategy but iterating multiple times. The key fact is that a new train/set distribution is set and a new model is estimated in each iteration. The only thing that remains constant is the value of the hold-out split.\n",
    "- It is preferable over a simple hold-out evaluation because produce less variable results, as the evaluation metrics (accuracy in this case) are averaged across the iterations.\n",
    "- Above it is the function definition called here (for consistency, all the functions are defined above). As it is expected, the value obtained for the accuracy has less variability. As we can check, for 10 samples, the accuracy goes between 0.31 and 0.41. But averaging this metric, with the Repeated Random Subsampling method the result has almost no variability, with values usually between 0.35 and 0.36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each sample: \n",
      "[0.36, 0.34, 0.42, 0.35, 0.32, 0.36, 0.34, 0.34, 0.31, 0.36]\n",
      "Average accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "# For 10 samples, this cell should last 10 seconds or less\n",
    "accuracy_samples, average = repeated_random_subsampling(10)\n",
    "print(\"Accuracy for each sample: \")\n",
    "print(accuracy_samples)\n",
    "print(\"Average accuracy: \"+ \"{:.2f}\".format(average) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: One-R Method\n",
    "\n",
    "- It consists on choosing the best attribute for predict an instance's class. For each attribute, is reviewed all its values: it is created a rule assigning to the value attribute the most frequent class. Finally, it is selected the attribute with the smallest error rate. \n",
    "- Above it is defined the function called here. The Naive Bayes classifier has, in general, a better performance. Even though is not a big difference (0.37 versus 0.35 in this final execution). The estimation for the One-R method still suffer of the variability of the hold-out strategy, even different features are chosen in each execution. However, usually the best features are \"Medu\" and \"Fedu\" (parent education). Obviously, in the context of this problem, this makes a lot of sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One R Method Feature: Fedu\n",
      "Accuraccy:0.36\n"
     ]
    }
   ],
   "source": [
    "selected_feature, error_rate = one_r_prediction(count,X_test,Y_test)\n",
    "print(\"One R Method Feature: \" + selected_feature)\n",
    "print(\"Accuraccy:\"+ \"{:.2f}\".format(1-error_rate) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
